# Visual Grounding on RefCOCOg with YOLO & CLIP

A visual grounding experiment for the Deep Learning course: given an image and a caption, we locate the correct region using two approaches:

1. **Baseline**  
   - Detect objects with YOLO (Ultralytics).  
   - Compute similarity between CLIP (RN50) vectors of each crop and the text.  
2. **ExtendedClip**  
   - Extend CLIP with a small trainable encoder on text and image features.  
   - Train/fine-tune on RefCOCOg to improve IoU and cosine similarity.

## ðŸ“‚ Structure

- **Environment Setup** â€“ install dependencies, imports, paths.  
- **Dataset & Metrics** â€“ custom dataset class for RefCOCOg + definitions of IoU, mIoU, cosine similarity.  
- **Baseline** â€“ YOLO+CLIP zero-shot.  
- **ExtendedClip** â€“ architecture with additional encoders, training loop, custom loss.  
- **Training & Evaluation** â€“ plots of loss and accuracy, metric comparison.  
- **Results** â€“ table of mIoU and cosine similarity.

## ðŸš€ Running on Colab

1. Go to **File â†’ Save a copy in Drive** to create your own Colab copy.  
2. Ensure GPU runtime: **Runtime â†’ Change runtime type â†’ GPU**.  
3. Run each cell in order (Shift + Enter) to install, prepare data, train and evaluate.

## ðŸ”§ Requirements (installed automatically on Colab)

```bash
!pip install torch torchvision clip ultralytics pycocotools textdistance matplotlib pillow numpy
